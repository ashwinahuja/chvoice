{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Normal Model, Equal Probs KD","provenance":[{"file_id":"1zG5IKsF_N7XN6303KURSTwPDS-fXXDVd","timestamp":1608855121334},{"file_id":"1o6iKyJFgp6oEG2hYhFKon78OZ_i-kqjM","timestamp":1608721963508},{"file_id":"146jjQdJyzVx3YW1nWj2MmAD1YIv2ZkHf","timestamp":1608595801361},{"file_id":"1kIERyYn_3e5Cy8wV1Jd5XRtKMTnKJ_bT","timestamp":1608590226653},{"file_id":"1qNcCtGAdQQ88szyJ5JMbU_Q4Dprm0WhJ","timestamp":1607859832096}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"91993c71bdf548bfbe05fcde360737c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9a1da3cbd9c9488f85aa4ec689fb10c7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d02a46753e99400c8f8c7e9439409ff4","IPY_MODEL_1612c10d0c8e46d3b1664321f7de8374"]}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwPjh3mhvTY7","executionInfo":{"status":"ok","timestamp":1608914552170,"user_tz":0,"elapsed":17782,"user":{"displayName":"Ashwin Ahuja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuKWIktrxmmhSOaRXRrVZsSt2agR2QVN2C66l6hA=s64","userId":"12850077665793280068"}},"outputId":"d5e30f02-4eb8-4871-d5e4-815ee76663cd"},"source":["!pip install torchaudio==0.7.0\n","!pip install librosa==0.8.0\n","!pip install git+https://github.com/indrasweb/chvoice"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchaudio==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/23/6b54106b3de029d3f10cf8debc302491c17630357449c900d6209665b302/torchaudio-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 11.9MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio==0.7.0) (1.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.8)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.7.0\n","Collecting librosa==0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/4d/c22d8ca74ca2c13cd4ac430fa353954886104321877b65fa871939e78591/librosa-0.8.0.tar.gz (183kB)\n","\u001b[K     |████████████████████████████████| 184kB 14.3MB/s \n","\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (2.1.9)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (1.19.4)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (1.4.1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (1.0.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (0.2.2)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.8.0) (0.48.0)\n","Collecting soundfile>=0.9.0\n","  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n","Collecting pooch>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b9/9876662636ba451d4406543047c0b45ca5b4e830f931308c8274dad1db43/pooch-1.3.0-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from resampy>=0.2.2->librosa==0.8.0) (1.15.0)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.8.0) (0.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.8.0) (51.0.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa==0.8.0) (1.14.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa==0.8.0) (20.8)\n","Collecting appdirs\n","  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa==0.8.0) (2.23.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0) (2.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pooch>=1.0->librosa==0.8.0) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa==0.8.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa==0.8.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa==0.8.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pooch>=1.0->librosa==0.8.0) (2020.12.5)\n","Building wheels for collected packages: librosa\n","  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for librosa: filename=librosa-0.8.0-cp36-none-any.whl size=201375 sha256=c1273888622b093ec9332506c9a85d7061a7d04455a78d2c4614b73155d8e22d\n","  Stored in directory: /root/.cache/pip/wheels/ee/10/1e/382bb4369e189938d5c02e06d10c651817da8d485bfd1647c9\n","Successfully built librosa\n","Installing collected packages: soundfile, appdirs, pooch, librosa\n","  Found existing installation: librosa 0.6.3\n","    Uninstalling librosa-0.6.3:\n","      Successfully uninstalled librosa-0.6.3\n","Successfully installed appdirs-1.4.4 librosa-0.8.0 pooch-1.3.0 soundfile-0.10.3.post1\n","Collecting git+https://github.com/indrasweb/chvoice\n","  Cloning https://github.com/indrasweb/chvoice to /tmp/pip-req-build-ax7e5uqp\n","  Running command git clone -q https://github.com/indrasweb/chvoice /tmp/pip-req-build-ax7e5uqp\n","Building wheels for collected packages: chvoice\n","  Building wheel for chvoice (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chvoice: filename=chvoice-0.0.1-cp36-none-any.whl size=5071 sha256=7ab964dcc41762c03968d6c0cc8a4fe86b67ce7d73a12b2b98e2f1aa05f0ee2e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e8fh3fbv/wheels/87/5a/6b/0a763689bb1840664ecf128003f345cbf2defc280c15bd393f\n","Successfully built chvoice\n","Installing collected packages: chvoice\n","Successfully installed chvoice-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWwTxpvyvdVw","executionInfo":{"status":"ok","timestamp":1608914556301,"user_tz":0,"elapsed":21889,"user":{"displayName":"Ashwin Ahuja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuKWIktrxmmhSOaRXRrVZsSt2agR2QVN2C66l6hA=s64","userId":"12850077665793280068"}},"outputId":"7f3dfaa0-c88a-4c8b-931e-87dc8053cfdc"},"source":["import torch as T\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio as ta\n","import torchvision\n","import numpy as np\n","from torchsummary import torchsummary\n","from IPython.display import Audio\n","import matplotlib.pyplot as plt\n","import random\n","import os\n","from glob import glob\n","import operator\n","from google.colab import drive\n","from torch.autograd import Variable\n","import chvoice\n","import pickle\n","import pandas as pd\n","import seaborn as sns\n","\n","sns.set()\n","sns.color_palette(\"viridis\", as_cmap=True)\n","sns.set_style(\"whitegrid\", {'axes.grid' : False})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n","  '\"sox\" backend is being deprecated. '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpbrecugnoOz","executionInfo":{"status":"ok","timestamp":1608914587625,"user_tz":0,"elapsed":53196,"user":{"displayName":"Ashwin Ahuja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuKWIktrxmmhSOaRXRrVZsSt2agR2QVN2C66l6hA=s64","userId":"12850077665793280068"}},"outputId":"8fb17654-46f6-4871-b13a-5de8594213ae"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"py0GBTeVvhjO","executionInfo":{"status":"ok","timestamp":1608914587626,"user_tz":0,"elapsed":53189,"user":{"displayName":"Ashwin Ahuja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuKWIktrxmmhSOaRXRrVZsSt2agR2QVN2C66l6hA=s64","userId":"12850077665793280068"}},"outputId":"a3dd2c94-2316-48ef-d104-c2d9c0b723ac"},"source":["DEVICE = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","DEVICE"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"CMZkhSsqyVbb"},"source":["# Experiment Configuration"]},{"cell_type":"code","metadata":{"id":"sQPtx9moyU1y"},"source":["experiment_save_path = pth = 'drive/MyDrive/chvoice-experiments-kd5/'\n","\n","if not os.path.exists(experiment_save_path):\n","    os.mkdir(experiment_save_path)\n","\n","wav_noise_config = {\n","    './noises/keyboard-mono': {'prob': 0.2, 'mul': 0.3},\n","    './noises/coughing-mono': {'prob': 0.2, 'mul': .1},\n","    './noises/clock-tick-mono': {'prob': 0.2, 'mul': .3},\n","    './noises/click-mono': {'prob': 0.2, 'mul': .3},\n","    './noises/wind-mono': {'prob': 0.2, 'mul': .1}\n","}\n","\n","additional_noise_config = {\n","    'white_noise': {'prob': 0.1, 'mul': 0.3},\n","    'no_noise': {'prob': 0.1},\n","    'reverb': {'prob': 0.1, 'density': 15}\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvninNKlG1MF"},"source":["Get Librispeech dataset, and some noise "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["91993c71bdf548bfbe05fcde360737c3"]},"id":"jmKwhRSCG6-y","outputId":"d5eeacee-2004-411d-b16f-de56843f62c5"},"source":["train_dataset = ta.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n","#train_dataset = ta.datasets.LIBRISPEECH(\"./\", url=\"dev-clean\", download=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91993c71bdf548bfbe05fcde360737c3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6387309499.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YG2Kxc2LG8WQ"},"source":["!curl -L -o noises.zip https://github.com/indrasweb/chvoice/raw/main/noises.zip\n","!unzip -q noises.zip\n","!rm -rf noises/.DS_Store"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQUDqM_bvjRs"},"source":["def preprocess(X, dsp, noiser):\n","    clean = []\n","    noisy = []\n","    wavs = [d[0] for d in X]\n","\n","    for wav in wavs:\n","\n","        db, phase = dsp.sig_to_db_phase(wav)\n","        if db.size(2) < 128:\n","            continue\n","\n","        # make clean chunks of audio\n","        chunks = db.unfold(2, 128, 128).squeeze(0).movedim(1,0)\n","        clean.append(chunks)\n","\n","        # make corresponding noisy chunks of audio\n","        aug_wav = noiser.add_noise(wav.squeeze(0))\n","        db, phase = dsp.sig_to_db_phase(aug_wav.unsqueeze(0))\n","        chunks = db.unfold(2, 128, 128).squeeze(0).movedim(1,0)\n","        noisy.append(chunks)\n","\n","    clean = T.vstack(clean).unsqueeze(1)\n","    noisy = T.vstack(noisy).unsqueeze(1)\n","    \n","    return clean, noisy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6R_raEN0S0I"},"source":["## Model Definition (UNet)"]},{"cell_type":"code","metadata":{"id":"BgB6S1yfSDwx"},"source":["class PrunableConv2d(nn.Conv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n","        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n","        self.taylor_estimates = None\n","        self._recent_activations = None\n","        self._pruning_hook = None\n","\n","    def forward(self, x):\n","        output = super().forward(x)\n","        self._recent_activations = output.clone()\n","        return output\n","\n","    def set_pruning_hooks(self):\n","        self._pruning_hook = self.register_backward_hook(self._calculate_taylor_estimate)\n","\n","    def _calculate_taylor_estimate(self, _, grad_input, grad_output):\n","        # skip dim 1 as it is kernel size\n","        estimates = self._recent_activations.mul_(grad_output[0])\n","        estimates = estimates.mean(dim=(0, 2, 3))        \n","\n","        # normalization\n","        self.taylor_estimates = T.abs(estimates) / T.sqrt(T.sum(estimates * estimates))\n","        del estimates, self._recent_activations\n","        self._recent_activations = None\n","\n","    def prune_feature_map(self, map_index):\n","        is_cuda = self.weight.is_cuda\n","\n","        indices = Variable(T.LongTensor([i for i in range(self.out_channels) if i != map_index]))\n","        indices = indices.cuda() if is_cuda else indices\n","\n","        self.weight = nn.Parameter(self.weight.index_select(0, indices).data)\n","        self.bias = nn.Parameter(self.bias.index_select(0, indices).data)\n","        self.out_channels -= 1\n","\n","    def drop_input_channel(self, index):\n","        is_cuda = self.weight.is_cuda\n","\n","        indices = Variable(torch.LongTensor([i for i in range(self.in_channels) if i != index]))\n","        indices = indices.cuda() if is_cuda else indices\n","\n","        self.weight = nn.Parameter(self.weight.index_select(1, indices).data)\n","        self.in_channels -= 1\n","\n","class PrunableBatchNorm2d(nn.BatchNorm2d):\n","  def __init__(self, channels):\n","      super(PrunableBatchNorm2d, self).__init__(channels)\n","\n","  def drop_input_channel(self, index):\n","        if self.affine:\n","            is_cuda = self.weight.is_cuda\n","            indices = Variable(torch.LongTensor([i for i in range(self.num_features) if i != index]))\n","            indices = indices.cuda() if is_cuda else indices\n","\n","            self.weight = nn.Parameter(self.weight.index_select(0, indices).data)\n","            self.bias = nn.Parameter(self.bias.index_select(0, indices).data)\n","            self.running_mean = self.running_mean.index_select(0, indices.data)\n","            self.running_var = self.running_var.index_select(0, indices.data)\n","\n","        self.num_features -= 1\n","\n","      \n","  \n","  def forward(self, x):\n","      x = super().forward(x)\n","      return x\n","      \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hfN0yhqSdPT"},"source":["class p_double_conv(nn.Module):\n","    def __init__(self, in_ch, out_ch, mid_channels=None):\n","        super(p_double_conv, self).__init__()\n","        \n","        if not mid_channels:\n","            mid_channels = out_ch\n","\n","        self.quant1 = T.quantization.QuantStub()\n","        self.dequant1 = T.quantization.DeQuantStub()\n","\n","        self.conv = nn.Sequential(\n","            PrunableConv2d(in_ch, out_ch, 3, padding=1),\n","            PrunableBatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            PrunableConv2d(mid_channels, out_ch, 3, padding=1),\n","            PrunableBatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","\n","\n","    def forward(self, x):\n","        x = self.quant1(x)\n","        x = self.conv(x)\n","        x = self.dequant1(x)\n","        return x\n","\n","\n","class p_inconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(p_inconv, self).__init__()\n","        self.conv = p_double_conv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class p_down(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(p_down, self).__init__()\n","        self.mpconv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            p_double_conv(in_ch, out_ch)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","        return x\n","\n","class p_up(nn.Module):\n","    def __init__(self, in_ch, out_ch, bilinear=True):\n","        super(p_up, self).__init__()\n","\n","        #  would be a nice idea if the upsampling could be learned too,\n","        #  but my machine do not have enough memory to handle all those weights\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = p_double_conv(in_ch, out_ch)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_ch , in_ch // 2, kernel_size=2, stride=2)\n","            self.conv = p_double_conv(in_ch, out_ch)\n","        \n","\n","      \n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        \n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        \n","        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2))\n","        # for padding issues, see \n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","\n","        x = T.cat([x2, x1], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","\n","class p_outconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(p_outconv, self).__init__()\n","        self.conv = PrunableConv2d(in_ch, out_ch, kernel_size=1, padding=0)\n","        \n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1I0LAPtSmfS"},"source":["class PruneUNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, channel_depth=64):\n","        super(PruneUNet, self).__init__()\n","        self.inc = p_inconv(n_channels, channel_depth)\n","        self.down1 = p_down(channel_depth, (channel_depth*2))\n","        self.down2 = p_down((channel_depth*2), (channel_depth*4))\n","        self.down3 = p_down((channel_depth*4), (channel_depth*8))\n","        self.down4 = p_down((channel_depth*8), (channel_depth*8))\n","        self.up1 = p_up(channel_depth*16, (channel_depth*4))\n","        self.up2 = p_up(channel_depth*8, (channel_depth*2))\n","        self.up3 = p_up(channel_depth*4, channel_depth)\n","        self.up4 = p_up(channel_depth*2, channel_depth)\n","        self.outc = p_outconv(channel_depth, n_classes)\n","\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        x = self.outc(x)\n","        return x\n","\n","    def set_pruning(self):\n","        prunable_modules = [module for module in self.modules()\n","                             if getattr(module, \"prune_feature_map\", False)\n","                             and module.out_channels > 1]\n","\n","        # # Print getting all conv layers\n","        # for i, m in enumerate(self.modules()):\n","        #     if getattr(m, \"prune_feature_map\", False) and m.out_channels > 1:\n","        #         print(i, \"->\", m)\n","\n","        for p in prunable_modules:\n","            p.set_pruning_hooks()\n","    \n","    def prune(self, verbose=False):\n","        # Get all layers excluding larger blocks\n","        module_list = [module for module in self.modules() \n","                        if not isinstance(module, \n","                            (nn.Sequential, \n","                            p_double_conv, \n","                            p_inconv, \n","                            p_down, \n","                            p_up, \n","                            p_outconv,\n","                            PruneUNet)\n","                            )\n","                        ]\n","        \n","        # Also checks if layer has been pruned to 1 channel remaining\n","        taylor_estimates_by_module = [(module.taylor_estimates, idx) for idx, module in enumerate(module_list) if getattr(module, \"prune_feature_map\", False) and module.out_channels > 1]\n","        taylor_estimates_by_feature_map = [(estimate, f_map_idx, module_idx) for estimates_by_f_map, module_idx in taylor_estimates_by_module for f_map_idx, estimate in enumerate(estimates_by_f_map)]\n","\n","        min_estimate, min_f_map_idx, min_module_idx = min(taylor_estimates_by_feature_map, key=operator.itemgetter(0))\n","\n","        p_conv = module_list[min_module_idx]\n","        p_conv.prune_feature_map(min_f_map_idx)\n","        \n","        if verbose:\n","            print(\"Pruned conv layer number {}, {}\".format(min_module_idx, p_conv))\n","\n","        # Find next conv layer to drop input channel\n","        is_last_conv = len(module_list)-1 == min_module_idx\n","        if not is_last_conv:\n","            p_batchnorm = module_list[min_module_idx+1]\n","            p_batchnorm.drop_input_channel(min_f_map_idx)\n","            \n","            next_conv_idx = min_module_idx + 2\n","            while next_conv_idx < len(module_list):\n","                if isinstance(module_list[next_conv_idx], PrunableConv2d):\n","                    module_list[next_conv_idx].drop_input_channel(min_f_map_idx)\n","                    if verbose:\n","                        print(\"Found next conv layer at number {}, {}\"\n","                            .format(next_conv_idx, module_list[next_conv_idx]))\n","                    break\n","                next_conv_idx += 1\n","            \n","            # Hardcoded way of dealing with up sampled layers\n","\n","            # x4 as output of down3 -> drop input channel of up1\n","            if min_module_idx == 24:\n","                p_up_conv = module_list[35]\n","                p_up_conv.drop_input_channel(min_f_map_idx)\n","            # x3 as output of down2 -> drop input channel of up2\n","            elif min_module_idx == 17:\n","                p_up_conv = module_list[42]\n","                p_up_conv.drop_input_channel(min_f_map_idx)\n","            # x2 as output of down1 -> drop input channel of up3\n","            elif min_module_idx == 10:\n","                p_up_conv = module_list[49]\n","                p_up_conv.drop_input_channel(min_f_map_idx)\n","            # x1 as output of inc -> drop input channel of up4\n","            elif min_module_idx == 3:\n","                p_up_conv = module_list[56]\n","                p_up_conv.drop_input_channel(min_f_map_idx)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eY2RDee_QUWe"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"IHX6eO8hQWQK"},"source":["def evaluate(noisy_sig, clean_sig, model, plot_spectrograms=True):\n","    \n","    db, phase = dsp.sig_to_db_phase(noisy_sig)\n","    assert db.size(2) >= 128, 'wav too small'\n","    db_clean, _ = dsp.sig_to_db_phase(clean_sig)\n","\n","    if plot_spectrograms:\n","        print('input dB spectrogram (noisy)')\n","        fig, ax = plt.subplots(figsize=(12,4))\n","        ax.imshow(db[0])\n","        plt.show()\n","\n","        print('target dB spectogram (clean)')\n","        fig, ax = plt.subplots(figsize=(12,4))\n","        ax.imshow(db_clean[0])\n","        plt.show()\n","\n","    chunks = db.unfold(2, 128, 128).squeeze(0).movedim(1,0)\n","    mean = T.mean(chunks)\n","    std = T.std(chunks) + 1e-6\n","    chunks = (chunks - mean) / std  # normalize\n","    chunks = chunks.unsqueeze(1).to(DEVICE)\n","                 \n","    with T.no_grad():\n","        proc = model(chunks)\n","    \n","    proc = (proc * std) + mean  # denormalize\n","    db_out = T.cat([c for c in proc.squeeze(1)], dim=1).cpu()\n","    \n","    if plot_spectrograms:\n","        print('db spectrogram (prediction)')\n","        fig, ax = plt.subplots(figsize=(12,4))\n","        ax.imshow(db_out.numpy())\n","        plt.show()\n","    \n","    phase_clipped = phase[0,:,:db_out.size(1)]\n","    sig = dsp.db_phase_to_sig(db_out, phase_clipped)\n","\n","    diff = db_out.numpy() - db[0].numpy()[:, :db_out.size(1)]\n","    if plot_spectrograms:\n","        print('noisy - prediciton (difference)')\n","        fig, ax = plt.subplots(figsize=(12,4))\n","        ax.imshow(diff)\n","        plt.show()\n","    \n","    print(\"Root mean squared difference\")\n","    print(np.sqrt(np.mean(diff*diff)))\n","    \n","    return sig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uyfex8YRIdW"},"source":["test_dataset = ta.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)\n","noiser = chvoice.Noiser(wav_noise_config, additional_noise_config)\n","dsp = chvoice.DSP()\n","\n","test_loader = T.utils.data.DataLoader(\n","    dataset=test_dataset,\n","    batch_size=15,\n","    shuffle=True,\n","    collate_fn=lambda x: preprocess(x, dsp, noiser),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHvv3SiKRQU4"},"source":["evaluation_in_sig = train_dataset[100][0]\n","Audio(evaluation_in_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwyqPcRpRUe7"},"source":["evaluation_wobbly = noiser.add_noise(evaluation_in_sig.squeeze(0)).unsqueeze(0)\n","Audio(evaluation_wobbly, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yz5_j9AxRXan"},"source":["reverber = chvoice.ReverbEcho(repeat=15)\n","evaluation_sig_reverb = reverber.reverb(evaluation_in_sig.unsqueeze(0)).squeeze(0)\n","Audio(evaluation_sig_reverb, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUO4nT1wgnLq"},"source":["def evaluate_whole_test_set(model):\n","    model = model.eval()\n","    avg_loss = 0\n","    c = 200\n","    crit = nn.MSELoss()\n","    for i, (clean, noisy) in enumerate(test_loader):\n","        clean = clean[:64,:,:,:].to(DEVICE)\n","        noisy = noisy[:64,:,:,:].to(DEVICE)\n","        # normalize\n","        mean = T.mean(noisy)\n","        std = T.std(noisy)\n","        noisy = (noisy - mean) / (std + 1e-6)\n","        optim.zero_grad()\n","        pred = model(noisy)\n","        # denormlaize\n","        pred = (pred * (std + 1e-6)) + mean\n","        loss = crit(pred, clean)\n","        avg_loss += loss.item()\n","        if i == c: break\n","    return avg_loss/c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GICov2PawOCN"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"Y_kARsYPwJRM"},"source":["train_loader = T.utils.data.DataLoader(\n","    dataset=train_dataset,\n","    batch_size=15,\n","    shuffle=True,\n","    collate_fn=lambda x: preprocess(x, dsp, noiser),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEQeRk6rpmPp"},"source":["## No Pruning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"3x5Qd---wNVj","executionInfo":{"elapsed":726859,"status":"error","timestamp":1608722748992,"user":{"displayName":"Ashwin Ahuja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuKWIktrxmmhSOaRXRrVZsSt2agR2QVN2C66l6hA=s64","userId":"12850077665793280068"},"user_tz":0},"outputId":"47311b9e-e2a2-49f1-a90a-567e9f9e1fc7"},"source":["model = PruneUNet(1, 1).to(DEVICE)\n","optim = T.optim.Adam(model.parameters(), 4e-4)\n","crit = nn.MSELoss()\n","loss_history = []\n","\n","for epoch in range(0, 5):\n","    print(f'----- EPOCH {epoch} -----')\n","    for i, (clean, noisy) in enumerate(train_loader):\n","        clean = clean[:64,:,:,:].to(DEVICE)\n","        noisy = noisy[:64,:,:,:].to(DEVICE)\n","        # normalize\n","        mean = T.mean(noisy)\n","        std = T.std(noisy)\n","        noisy = (noisy - mean) / (std + 1e-6)\n","        optim.zero_grad()\n","        pred = model(noisy)\n","        # denormlaize\n","        pred = (pred * (std + 1e-6)) + mean\n","        loss = crit(pred, clean)\n","        loss.backward()\n","        optim.step()\n","        loss_history.append(loss.item())\n","        if not i%10:\n","          print(f'E{epoch}:{i}\\tLoss {loss.item():.4f}')\n","    \n","    with open(pth+'loss-history-no-pruning.pkl', 'wb') as f:\n","        pickle.dump(loss_history, f)\n","    T.save(model.state_dict(), pth+f'model-no-pruning-{epoch}.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----- EPOCH 0 -----\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n","  normalized, onesided, return_complex)\n","/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n","  normalized, onesided, return_complex)\n"],"name":"stderr"},{"output_type":"stream","text":["E0:0\tLoss 455.0069\n","E0:10\tLoss 98.0499\n","E0:20\tLoss 18.9567\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-22866d33203a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'E{epoch}:{i}\\tLoss {loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"hm2ezciCRzBo"},"source":["df = pd.DataFrame(loss_history).rolling(250).mean()\n","df.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XW-NDsakSCr7"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_wobbly, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtLDrn8YRcO5"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_sig_reverb, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjL0YkOVjNnF"},"source":["test_loss = evaluate_whole_test_set(model)\n","print(f'Test MSE {test_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmsYcoEMppmm"},"source":["## With Pruning"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QXqhzw6HS-MO","outputId":"f232ee47-d5c0-4889-e9f5-e7025611ee94"},"source":["model = PruneUNet(n_channels=1, n_classes=1).to(DEVICE)\n","optim = T.optim.Adam(model.parameters(), 4e-4)\n","crit = nn.MSELoss()\n","loss_history = []\n","model.set_pruning()\n","counter = 0\n","\n","for epoch in range(1, 5):\n","    print(f'----- EPOCH {epoch} -----')\n","    for i, (clean, noisy) in enumerate(train_loader):\n","        clean = clean[:64,:,:,:].to(DEVICE)\n","        noisy = noisy[:64,:,:,:].to(DEVICE)\n","        # normalize\n","        mean = T.mean(noisy)\n","        std = T.std(noisy)\n","        noisy = (noisy - mean) / (std + 1e-6)\n","        optim.zero_grad()\n","\n","        try:\n","            pred = model(noisy)\n","        except:\n","            print(\"Pruning broke something\")\n","            counter += 1\n","            model = T.load(pth+f'raw_model')\n","            pred = model(noisy)\n","\n","        # denormlaize\n","        pred = (pred * (std + 1e-6)) + mean\n","        loss = crit(pred, clean)\n","        loss.backward()\n","        optim.step()\n","        loss_history.append(loss.item())\n","\n","        if not i%10:\n","            print(f'E{epoch}:{i}\\tLoss {loss.item():.4f}')\n","\n","    T.save(model, pth+f'raw_model')\n","    \n","    for j in range(30):\n","        model.prune(verbose=True) \n","        \n","    with open(pth+'loss-history-with-pruning.pkl', 'wb') as f:\n","        pickle.dump(loss_history, f)\n","    T.save(model.state_dict(), pth+f'model-with-pruning-{epoch}.pkl')\n","\n","print(\"Pruning failed: \" + str(counter))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----- EPOCH 1 -----\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n","  normalized, onesided, return_complex)\n","/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n","  normalized, onesided, return_complex)\n"],"name":"stderr"},{"output_type":"stream","text":["E1:0\tLoss 74.4338\n","E1:10\tLoss 227.8901\n","E1:20\tLoss 15.9352\n","E1:30\tLoss 28.6848\n","E1:40\tLoss 42.3281\n","E1:50\tLoss 20.0882\n","E1:60\tLoss 36.5353\n","E1:70\tLoss 36.2313\n","E1:80\tLoss 19.0719\n","E1:90\tLoss 25.1662\n","E1:100\tLoss 232.1343\n","E1:110\tLoss 92.9599\n","E1:120\tLoss 26.2489\n","E1:130\tLoss 27.9842\n","E1:140\tLoss 62.3737\n","E1:150\tLoss 221.6167\n","E1:160\tLoss 8.7156\n","E1:170\tLoss 13.1541\n","E1:180\tLoss 67.7973\n","E1:190\tLoss 9.0868\n","E1:200\tLoss 33.1393\n","E1:210\tLoss 36.5235\n","E1:220\tLoss 27.5585\n","E1:230\tLoss 17.1350\n","E1:240\tLoss 79.8577\n","E1:250\tLoss 195.8305\n","E1:260\tLoss 8.9814\n","E1:270\tLoss 160.6657\n","E1:280\tLoss 13.2974\n","E1:290\tLoss 17.6676\n","E1:300\tLoss 4.8588\n","E1:310\tLoss 32.5264\n","E1:320\tLoss 16.4166\n","E1:330\tLoss 5.1382\n","E1:340\tLoss 23.6864\n","E1:350\tLoss 5.7161\n","E1:360\tLoss 25.4545\n","E1:370\tLoss 84.8556\n","E1:380\tLoss 6.3295\n","E1:390\tLoss 29.1434\n","E1:400\tLoss 6.7129\n","E1:410\tLoss 45.4953\n","E1:420\tLoss 9.6131\n","E1:430\tLoss 10.8249\n","E1:440\tLoss 9.8598\n","E1:450\tLoss 38.9332\n","E1:460\tLoss 32.7305\n","E1:470\tLoss 39.4330\n","E1:480\tLoss 38.1242\n","E1:490\tLoss 22.1974\n","E1:500\tLoss 64.1985\n","E1:510\tLoss 18.8637\n","E1:520\tLoss 12.7004\n","E1:530\tLoss 43.0538\n","E1:540\tLoss 23.2487\n","E1:550\tLoss 21.7476\n","E1:560\tLoss 11.0747\n","E1:570\tLoss 26.5956\n","E1:580\tLoss 15.6902\n","E1:590\tLoss 13.8076\n","E1:600\tLoss 5.1306\n","E1:610\tLoss 7.5963\n","E1:620\tLoss 13.3620\n","E1:630\tLoss 8.7197\n","E1:640\tLoss 25.8121\n","E1:650\tLoss 28.2238\n","E1:660\tLoss 82.6758\n","E1:670\tLoss 27.4412\n","E1:680\tLoss 5.4418\n","E1:690\tLoss 20.5920\n","E1:700\tLoss 27.7458\n","E1:710\tLoss 20.5996\n","E1:720\tLoss 19.1300\n","E1:730\tLoss 19.3728\n","E1:740\tLoss 11.7297\n","E1:750\tLoss 15.1952\n","E1:760\tLoss 13.6453\n","E1:770\tLoss 26.4308\n","E1:780\tLoss 25.4502\n","E1:790\tLoss 5.9834\n","E1:800\tLoss 15.2146\n","E1:810\tLoss 6.9242\n","E1:820\tLoss 15.7534\n","E1:830\tLoss 17.4270\n","E1:840\tLoss 10.5487\n","E1:850\tLoss 6.4810\n","E1:860\tLoss 26.2422\n","E1:870\tLoss 9.1549\n","E1:880\tLoss 19.1542\n","E1:890\tLoss 157.7121\n","E1:900\tLoss 12.5762\n","E1:910\tLoss 4.9330\n","E1:920\tLoss 22.8885\n","E1:930\tLoss 19.8404\n","E1:940\tLoss 81.8779\n","E1:950\tLoss 13.4741\n","E1:960\tLoss 47.5522\n","E1:970\tLoss 12.1974\n","E1:980\tLoss 38.6018\n","E1:990\tLoss 33.9191\n","E1:1000\tLoss 11.6883\n","E1:1010\tLoss 12.9919\n","E1:1020\tLoss 6.3749\n","E1:1030\tLoss 5.5888\n","E1:1040\tLoss 20.5249\n","E1:1050\tLoss 26.3745\n","E1:1060\tLoss 58.3677\n","E1:1070\tLoss 24.9305\n","E1:1080\tLoss 11.9099\n","E1:1090\tLoss 28.0172\n","E1:1100\tLoss 23.7638\n","E1:1110\tLoss 11.4418\n","E1:1120\tLoss 43.5257\n","E1:1130\tLoss 20.2392\n","E1:1140\tLoss 23.3006\n","E1:1150\tLoss 418.4518\n","E1:1160\tLoss 18.1672\n","E1:1170\tLoss 33.4105\n","E1:1180\tLoss 31.1880\n","E1:1190\tLoss 18.8768\n","E1:1200\tLoss 44.9442\n","E1:1210\tLoss 24.3361\n","E1:1220\tLoss 15.8117\n","E1:1230\tLoss 7.5995\n","E1:1240\tLoss 72.0598\n","E1:1250\tLoss 25.0819\n","E1:1260\tLoss 30.0605\n","E1:1270\tLoss 17.1035\n","E1:1280\tLoss 23.3998\n","E1:1290\tLoss 37.3477\n","E1:1300\tLoss 14.0351\n","E1:1310\tLoss 23.7638\n","E1:1320\tLoss 37.6919\n","E1:1330\tLoss 46.7145\n","E1:1340\tLoss 30.3271\n","E1:1350\tLoss 15.5790\n","E1:1360\tLoss 8.2618\n","E1:1370\tLoss 30.1407\n","E1:1380\tLoss 64.2341\n","E1:1390\tLoss 26.2020\n","E1:1400\tLoss 14.9464\n","E1:1410\tLoss 26.5891\n","E1:1420\tLoss 18.1435\n","E1:1430\tLoss 29.1558\n","E1:1440\tLoss 13.8317\n","E1:1450\tLoss 17.4307\n","E1:1460\tLoss 15.6820\n","E1:1470\tLoss 52.9674\n","E1:1480\tLoss 13.9727\n","E1:1490\tLoss 18.2852\n","E1:1500\tLoss 24.5722\n","E1:1510\tLoss 34.7872\n","E1:1520\tLoss 10.7694\n","E1:1530\tLoss 51.5640\n","E1:1540\tLoss 19.8536\n","E1:1550\tLoss 20.3292\n","E1:1560\tLoss 53.3335\n","E1:1570\tLoss 10.8584\n","E1:1580\tLoss 53.1664\n","E1:1590\tLoss 68.5046\n","E1:1600\tLoss 12.3222\n","E1:1610\tLoss 15.5805\n","E1:1620\tLoss 9.3100\n","E1:1630\tLoss 36.4487\n","E1:1640\tLoss 27.1757\n","E1:1650\tLoss 7.9604\n","E1:1660\tLoss 33.4975\n","E1:1670\tLoss 7.6354\n","E1:1680\tLoss 6.1469\n","E1:1690\tLoss 22.4570\n","E1:1700\tLoss 15.6361\n","E1:1710\tLoss 7.7195\n","E1:1720\tLoss 15.4052\n","E1:1730\tLoss 31.1062\n","E1:1740\tLoss 27.4058\n","E1:1750\tLoss 50.2449\n","E1:1760\tLoss 28.8918\n","E1:1770\tLoss 11.5462\n","E1:1780\tLoss 49.1769\n","E1:1790\tLoss 13.4424\n","E1:1800\tLoss 76.2793\n","E1:1810\tLoss 51.1292\n","E1:1820\tLoss 33.0271\n","E1:1830\tLoss 5.5816\n","E1:1840\tLoss 11.2110\n","E1:1850\tLoss 28.8163\n","E1:1860\tLoss 32.7863\n","E1:1870\tLoss 10.1548\n","E1:1880\tLoss 15.1438\n","E1:1890\tLoss 25.1389\n","E1:1900\tLoss 23.7750\n","Pruned conv layer number 20, PrunableConv2d(128, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(255, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(254, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 253, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(253, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(252, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 251, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(251, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(250, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 249, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(249, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(248, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 247, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(247, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 246, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(246, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 245, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(245, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 244, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(244, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 243, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(243, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 242, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(242, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 241, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(241, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(240, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 239, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(239, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(238, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 237, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(237, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 236, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(236, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 235, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(235, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 234, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(234, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 233, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(233, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(232, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(231, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 230, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(230, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 229, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(229, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 228, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(228, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 227, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(227, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 20, PrunableConv2d(128, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 23, PrunableConv2d(226, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","----- EPOCH 2 -----\n","E2:0\tLoss 19.3930\n","E2:10\tLoss 403.8611\n","E2:20\tLoss 42.0011\n","E2:30\tLoss 76.5233\n","E2:40\tLoss 27.4909\n","E2:50\tLoss 54.1180\n","E2:60\tLoss 17.5040\n","E2:70\tLoss 66.4313\n","E2:80\tLoss 19.9781\n","E2:90\tLoss 13.9494\n","E2:100\tLoss 9.5599\n","E2:110\tLoss 10.4063\n","E2:120\tLoss 19.4387\n","E2:130\tLoss 43.5966\n","E2:140\tLoss 15.1599\n","E2:150\tLoss 78.5549\n","E2:160\tLoss 50.3952\n","E2:170\tLoss 10.9900\n","E2:180\tLoss 37.2517\n","E2:190\tLoss 28.2417\n","E2:200\tLoss 26.9899\n","E2:210\tLoss 42.4135\n","E2:220\tLoss 6.7902\n","E2:230\tLoss 11.6340\n","E2:240\tLoss 28.0860\n","E2:250\tLoss 160.3744\n","E2:260\tLoss 50.9771\n","E2:270\tLoss 99.1732\n","E2:280\tLoss 4.9220\n","E2:290\tLoss 33.6246\n","E2:300\tLoss 32.5617\n","E2:310\tLoss 9.7971\n","E2:320\tLoss 22.2906\n","E2:330\tLoss 27.5800\n","E2:340\tLoss 5.6302\n","E2:350\tLoss 30.7318\n","E2:360\tLoss 17.8158\n","E2:370\tLoss 46.4246\n","E2:380\tLoss 21.4667\n","E2:390\tLoss 12.0153\n","E2:400\tLoss 10.4721\n","E2:410\tLoss 24.6989\n","E2:420\tLoss 18.5344\n","E2:430\tLoss 13.2405\n","E2:440\tLoss 28.8822\n","E2:450\tLoss 16.1843\n","E2:460\tLoss 22.0867\n","E2:470\tLoss 28.9545\n","E2:480\tLoss 34.6827\n","E2:490\tLoss 9.3315\n","E2:500\tLoss 25.0427\n","E2:510\tLoss 25.1523\n","E2:520\tLoss 26.1537\n","E2:530\tLoss 12.3516\n","E2:540\tLoss 9.1908\n","E2:550\tLoss 22.9090\n","E2:560\tLoss 41.1831\n","E2:570\tLoss 24.9869\n","E2:580\tLoss 16.8982\n","E2:590\tLoss 7.5552\n","E2:600\tLoss 2.9763\n","E2:610\tLoss 302.2935\n","E2:620\tLoss 78.2945\n","E2:630\tLoss 15.5855\n","E2:640\tLoss 12.1710\n","E2:650\tLoss 30.0616\n","E2:660\tLoss 39.5359\n","E2:670\tLoss 29.8634\n","E2:680\tLoss 35.3446\n","E2:690\tLoss 28.6271\n","E2:700\tLoss 8.6367\n","E2:710\tLoss 19.2635\n","E2:720\tLoss 7.3322\n","E2:730\tLoss 245.7149\n","E2:740\tLoss 13.4646\n","E2:750\tLoss 19.6791\n","E2:760\tLoss 39.2121\n","E2:770\tLoss 19.3222\n","E2:780\tLoss 79.5982\n","E2:790\tLoss 23.0775\n","E2:800\tLoss 35.2138\n","E2:810\tLoss 21.8527\n","E2:820\tLoss 40.3959\n","E2:830\tLoss 18.6532\n","E2:840\tLoss 116.2202\n","E2:850\tLoss 38.5325\n","E2:860\tLoss 6.0955\n","E2:870\tLoss 21.9969\n","E2:880\tLoss 22.5263\n","E2:890\tLoss 60.2660\n","E2:900\tLoss 17.0971\n","E2:910\tLoss 26.0437\n","E2:920\tLoss 11.1168\n","E2:930\tLoss 34.0088\n","E2:940\tLoss 25.8105\n","E2:950\tLoss 16.5034\n","E2:960\tLoss 14.7790\n","E2:970\tLoss 9.5883\n","E2:980\tLoss 40.1157\n","E2:990\tLoss 8.7350\n","E2:1000\tLoss 76.2593\n","E2:1010\tLoss 18.9316\n","E2:1020\tLoss 17.5361\n","E2:1030\tLoss 7.5251\n","E2:1040\tLoss 46.7051\n","E2:1050\tLoss 54.9361\n","E2:1060\tLoss 31.9775\n","E2:1070\tLoss 4.8809\n","E2:1080\tLoss 33.9903\n","E2:1090\tLoss 107.4119\n","E2:1100\tLoss 13.5165\n","E2:1110\tLoss 27.8484\n","E2:1120\tLoss 6.3570\n","E2:1130\tLoss 22.6979\n","E2:1140\tLoss 11.0784\n","E2:1150\tLoss 28.7324\n","E2:1160\tLoss 28.0067\n","E2:1170\tLoss 22.5963\n","E2:1180\tLoss 13.1238\n","E2:1190\tLoss 8.1668\n","E2:1200\tLoss 39.2034\n","E2:1210\tLoss 7.8471\n","E2:1220\tLoss 26.9415\n","E2:1230\tLoss 14.5305\n","E2:1240\tLoss 24.4908\n","E2:1250\tLoss 17.5535\n","E2:1260\tLoss 35.5331\n","E2:1270\tLoss 26.7479\n","E2:1280\tLoss 35.0510\n","E2:1290\tLoss 20.4032\n","E2:1300\tLoss 52.4171\n","E2:1310\tLoss 8.7885\n","E2:1320\tLoss 4.3430\n","E2:1330\tLoss 8.9304\n","E2:1340\tLoss 4.8625\n","E2:1350\tLoss 25.4228\n","E2:1360\tLoss 10.3767\n","E2:1370\tLoss 12.8015\n","E2:1380\tLoss 3.7563\n","E2:1390\tLoss 22.2526\n","E2:1400\tLoss 192.3937\n","E2:1410\tLoss 16.6508\n","E2:1420\tLoss 20.8505\n","E2:1430\tLoss 8.2380\n","E2:1440\tLoss 15.8025\n","E2:1450\tLoss 27.7772\n","E2:1460\tLoss 13.5443\n","E2:1470\tLoss 23.0759\n","E2:1480\tLoss 7.9585\n","E2:1490\tLoss 7.2408\n","E2:1500\tLoss 21.6605\n","E2:1510\tLoss 9.1627\n","E2:1520\tLoss 21.4387\n","E2:1530\tLoss 5.1889\n","E2:1540\tLoss 36.8401\n","E2:1550\tLoss 4.4451\n","E2:1560\tLoss 22.9403\n","E2:1570\tLoss 182.8196\n","E2:1580\tLoss 8.5949\n","E2:1590\tLoss 19.3261\n","E2:1600\tLoss 30.7628\n","E2:1610\tLoss 14.7241\n","E2:1620\tLoss 10.4376\n","E2:1630\tLoss 9.0294\n","E2:1640\tLoss 12.7637\n","E2:1650\tLoss 24.7064\n","E2:1660\tLoss 20.2189\n","E2:1670\tLoss 7.4273\n","E2:1680\tLoss 4.3352\n","E2:1690\tLoss 25.6574\n","E2:1700\tLoss 5.4571\n","E2:1710\tLoss 5.3228\n","E2:1720\tLoss 15.9313\n","E2:1730\tLoss 185.3427\n","E2:1740\tLoss 7.1746\n","E2:1750\tLoss 18.5972\n","E2:1760\tLoss 52.9935\n","E2:1770\tLoss 14.7193\n","E2:1780\tLoss 52.7350\n","E2:1790\tLoss 12.8712\n","E2:1800\tLoss 19.8546\n","E2:1810\tLoss 11.7897\n","E2:1820\tLoss 5.5196\n","E2:1830\tLoss 40.9504\n","E2:1840\tLoss 7.1843\n","E2:1850\tLoss 12.5745\n","E2:1860\tLoss 12.1755\n","E2:1870\tLoss 7.1477\n","E2:1880\tLoss 4.2430\n","E2:1890\tLoss 7.7185\n","E2:1900\tLoss 14.5274\n","Pruned conv layer number 14, PrunableConv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(127, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(126, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(125, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(124, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 123, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(123, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(122, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(121, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(120, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(119, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(118, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 117, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(117, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(116, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(115, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(114, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 113, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(113, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(112, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(111, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(110, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 109, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(109, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(108, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(107, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(106, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 105, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(105, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(104, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 103, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(103, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(102, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(101, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(100, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(99, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(98, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","----- EPOCH 3 -----\n","Pruning broke something\n","E3:0\tLoss 13.9086\n","E3:10\tLoss 9.4212\n","E3:20\tLoss 112.7920\n","E3:30\tLoss 52.4586\n","E3:40\tLoss 159.4429\n","E3:50\tLoss 9.8755\n","E3:60\tLoss 11.5467\n","E3:70\tLoss 32.8871\n","E3:80\tLoss 6.6160\n","E3:90\tLoss 8.8066\n","E3:100\tLoss 6.8087\n","E3:110\tLoss 28.9529\n","E3:120\tLoss 28.3511\n","E3:130\tLoss 27.4120\n","E3:140\tLoss 38.6871\n","E3:150\tLoss 19.2967\n","E3:160\tLoss 6.0382\n","E3:170\tLoss 23.7103\n","E3:180\tLoss 17.7464\n","E3:190\tLoss 163.6241\n","E3:200\tLoss 8.0397\n","E3:210\tLoss 10.8979\n","E3:220\tLoss 7.7368\n","E3:230\tLoss 12.3227\n","E3:240\tLoss 183.4680\n","E3:250\tLoss 41.0625\n","E3:260\tLoss 109.7336\n","E3:270\tLoss 11.6641\n","E3:280\tLoss 40.1747\n","E3:290\tLoss 19.3997\n","E3:300\tLoss 14.8088\n","E3:310\tLoss 23.6984\n","E3:320\tLoss 12.4365\n","E3:330\tLoss 82.8334\n","E3:340\tLoss 28.3269\n","E3:350\tLoss 36.4268\n","E3:360\tLoss 115.8089\n","E3:370\tLoss 19.3747\n","E3:380\tLoss 16.5289\n","E3:390\tLoss 13.4680\n","E3:400\tLoss 21.3592\n","E3:410\tLoss 38.2354\n","E3:420\tLoss 10.8165\n","E3:430\tLoss 23.2139\n","E3:440\tLoss 32.9454\n","E3:450\tLoss 12.8988\n","E3:460\tLoss 8.8166\n","E3:470\tLoss 44.7020\n","E3:480\tLoss 11.2320\n","E3:490\tLoss 35.1826\n","E3:500\tLoss 12.2492\n","E3:510\tLoss 22.5210\n","E3:520\tLoss 17.3760\n","E3:530\tLoss 8.0861\n","E3:540\tLoss 6.5926\n","E3:550\tLoss 11.7576\n","E3:560\tLoss 29.2417\n","E3:570\tLoss 17.8426\n","E3:580\tLoss 31.6111\n","E3:590\tLoss 14.8114\n","E3:600\tLoss 53.4772\n","E3:610\tLoss 18.1322\n","E3:620\tLoss 7.9885\n","E3:630\tLoss 45.4877\n","E3:640\tLoss 12.8186\n","E3:650\tLoss 27.7539\n","E3:660\tLoss 18.2940\n","E3:670\tLoss 31.0031\n","E3:680\tLoss 7.6519\n","E3:690\tLoss 32.6981\n","E3:700\tLoss 12.6457\n","E3:710\tLoss 21.6680\n","E3:720\tLoss 7.5453\n","E3:730\tLoss 19.3549\n","E3:740\tLoss 32.5221\n","E3:750\tLoss 34.5845\n","E3:760\tLoss 34.9916\n","E3:770\tLoss 25.3224\n","E3:780\tLoss 13.1638\n","E3:790\tLoss 26.2007\n","E3:800\tLoss 44.1148\n","E3:810\tLoss 26.3984\n","E3:820\tLoss 17.9433\n","E3:830\tLoss 25.0222\n","E3:840\tLoss 54.9553\n","E3:850\tLoss 312.0278\n","E3:860\tLoss 28.5932\n","E3:870\tLoss 32.4221\n","E3:880\tLoss 32.4464\n","E3:890\tLoss 9.7408\n","E3:900\tLoss 12.9123\n","E3:910\tLoss 19.9506\n","E3:920\tLoss 23.6502\n","E3:930\tLoss 31.1923\n","E3:940\tLoss 13.7140\n","E3:950\tLoss 27.4074\n","E3:960\tLoss 13.5112\n","E3:970\tLoss 19.9325\n","E3:980\tLoss 10.4408\n","E3:990\tLoss 184.1408\n","E3:1000\tLoss 22.8676\n","E3:1010\tLoss 40.7228\n","E3:1020\tLoss 11.7345\n","E3:1030\tLoss 30.2484\n","E3:1040\tLoss 21.6629\n","E3:1050\tLoss 28.7320\n","E3:1060\tLoss 15.5185\n","E3:1070\tLoss 21.8237\n","E3:1080\tLoss 9.5720\n","E3:1090\tLoss 8.3619\n","E3:1100\tLoss 9.6266\n","E3:1110\tLoss 27.7444\n","E3:1120\tLoss 18.8702\n","E3:1130\tLoss 12.3167\n","E3:1140\tLoss 38.4957\n","E3:1150\tLoss 12.2572\n","E3:1160\tLoss 15.6413\n","E3:1170\tLoss 107.1883\n","E3:1180\tLoss 37.8362\n","E3:1190\tLoss 22.3358\n","E3:1200\tLoss 6.6862\n","E3:1210\tLoss 9.4473\n","E3:1220\tLoss 16.1935\n","E3:1230\tLoss 16.5138\n","E3:1240\tLoss 20.2594\n","E3:1250\tLoss 11.5361\n","E3:1260\tLoss 17.9642\n","E3:1270\tLoss 64.7105\n","E3:1280\tLoss 19.1996\n","E3:1290\tLoss 31.6785\n","E3:1300\tLoss 23.7297\n","E3:1310\tLoss 8.9542\n","E3:1320\tLoss 35.4934\n","E3:1330\tLoss 21.4067\n","E3:1340\tLoss 16.6819\n","E3:1350\tLoss 25.7799\n","E3:1360\tLoss 6.1798\n","E3:1370\tLoss 7.6581\n","E3:1380\tLoss 14.7522\n","E3:1390\tLoss 16.3835\n","E3:1400\tLoss 18.2332\n","E3:1410\tLoss 12.7615\n","E3:1420\tLoss 10.3645\n","E3:1430\tLoss 25.0219\n","E3:1440\tLoss 100.4205\n","E3:1450\tLoss 6.8503\n","E3:1460\tLoss 16.0058\n","E3:1470\tLoss 9.1456\n","E3:1480\tLoss 7.4449\n","E3:1490\tLoss 13.3650\n","E3:1500\tLoss 18.6289\n","E3:1510\tLoss 75.6599\n","E3:1520\tLoss 23.3448\n","E3:1530\tLoss 16.2092\n","E3:1540\tLoss 18.1336\n","E3:1550\tLoss 44.3432\n","E3:1560\tLoss 28.1882\n","E3:1570\tLoss 29.3559\n","E3:1580\tLoss 11.7077\n","E3:1590\tLoss 18.3127\n","E3:1600\tLoss 122.0199\n","E3:1610\tLoss 30.3347\n","E3:1620\tLoss 23.7060\n","E3:1630\tLoss 14.0067\n","E3:1640\tLoss 21.3392\n","E3:1650\tLoss 21.5767\n","E3:1660\tLoss 22.3787\n","E3:1670\tLoss 79.7777\n","E3:1680\tLoss 79.5686\n","E3:1690\tLoss 8.1735\n","E3:1700\tLoss 107.2739\n","E3:1710\tLoss 10.6057\n","E3:1720\tLoss 79.6875\n","E3:1730\tLoss 29.0510\n","E3:1740\tLoss 56.2590\n","E3:1750\tLoss 12.1014\n","E3:1760\tLoss 11.0106\n","E3:1770\tLoss 71.6137\n","E3:1780\tLoss 22.3321\n","E3:1790\tLoss 27.8683\n","E3:1800\tLoss 14.9086\n","E3:1810\tLoss 20.5129\n","E3:1820\tLoss 6.3500\n","E3:1830\tLoss 368.6536\n","E3:1840\tLoss 38.3252\n","E3:1850\tLoss 35.2821\n","E3:1860\tLoss 16.8075\n","E3:1870\tLoss 65.9027\n","E3:1880\tLoss 8.3863\n","E3:1890\tLoss 14.5852\n","E3:1900\tLoss 28.2473\n","Pruned conv layer number 14, PrunableConv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(127, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(126, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(125, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(124, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 123, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(123, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(122, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(121, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(120, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(119, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(118, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 117, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(117, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(116, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(115, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(114, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 113, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(113, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(112, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(111, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(110, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 109, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(109, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(108, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(107, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(106, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 105, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(105, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(104, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 103, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(103, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(102, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(101, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(100, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(99, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Pruned conv layer number 14, PrunableConv2d(128, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","Found next conv layer at number 20, PrunableConv2d(98, 226, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","----- EPOCH 4 -----\n","Pruning broke something\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-bef208f1967a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-11-c2a45265b270>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-10-e17dfcc9f8bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-10-e17dfcc9f8bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequant1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-9-69c80a01ef3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recent_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 256, 3, 3], expected input[64, 226, 64, 64] to have 256 channels, but got 226 channels instead","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-bef208f1967a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'raw_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# denormlaize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-c2a45265b270>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e17dfcc9f8bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e17dfcc9f8bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequant1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-69c80a01ef3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 14.73 GiB total capacity; 13.30 GiB already allocated; 195.88 MiB free; 13.60 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"1LAe3rQESPCU"},"source":["df = pd.DataFrame(loss_history).rolling(250).mean()\n","df.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"UFyc9VvySPCU"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_wobbly, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"dZFDr70dSPCU"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_sig_reverb, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"7jKda5L_jX7s"},"source":["test_loss = evaluate_whole_test_set(model)\n","print(f'Test MSE {test_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JO0EwuXzOHRt"},"source":["## Knowledge Distillation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhLA-F3u3bwD","outputId":"74fddbcb-8a21-4296-b234-85cc14503d03"},"source":["model = PruneUNet(n_channels=1, n_classes=1).to(DEVICE).eval()\n","model.load_state_dict(T.load('./model.pkl'))\n","student = PruneUNet(n_channels=1, n_classes=1, channel_depth=32).to(DEVICE)\n","student.load_state_dict(T.load('./model-prev.pkl'))\n","\n","optim = T.optim.Adam(student.parameters(), 4e-4)\n","crit = nn.MSELoss()\n","loss_history = []\n","\n","\n","for epoch in range(4, 5):\n","    print(f'----- EPOCH {epoch} -----')\n","\n","    for i, (clean, noisy) in enumerate(train_loader):\n","        # clean = clean[:64,:,:,:].to(DEVICE) # Don't need!\n","        noisy = noisy[:64,:,:,:].to(DEVICE)\n","\n","        mean = T.mean(noisy)\n","        std = T.std(noisy)\n","        noisy = (noisy - mean) / (std + 1e-6)\n","\n","        model_output = model(noisy)\n","        # denormlaize\n","        model_output = (model_output * (std + 1e-6)) + mean\n","\n","        optim.zero_grad()\n","        pred = student(noisy)\n","        pred = (pred * (std + 1e-6)) + mean\n","\n","        loss = crit(pred, model_output)\n","        loss.backward()\n","        optim.step()\n","\n","        if not i%10:\n","            print(f'E{epoch}:{i}\\tLoss {loss.item():.4f}')\n","\n","    with open(pth+'loss-history-distillation.pkl', 'wb') as f:\n","        pickle.dump(loss_history, f)\n","    T.save(student.state_dict(), pth+f'model-distilled-{epoch}.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----- EPOCH 0 -----\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n","  normalized, onesided, return_complex)\n","/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n","  normalized, onesided, return_complex)\n"],"name":"stderr"},{"output_type":"stream","text":["E0:0\tLoss 78.7507\n","E0:10\tLoss 11.6560\n","E0:20\tLoss 4.2955\n","E0:30\tLoss 7.0241\n","E0:40\tLoss 7.0401\n","E0:50\tLoss 5.8079\n","E0:60\tLoss 7.3519\n","E0:70\tLoss 12.3889\n","E0:80\tLoss 12.1473\n","E0:90\tLoss 10.5663\n","E0:100\tLoss 8.1648\n","E0:110\tLoss 4.5746\n","E0:120\tLoss 12.5859\n","E0:130\tLoss 5.3705\n","E0:140\tLoss 13.0784\n","E0:150\tLoss 17.0687\n","E0:160\tLoss 11.7730\n","E0:170\tLoss 6.1909\n","E0:180\tLoss 8.0106\n","E0:190\tLoss 15.2208\n","E0:200\tLoss 25.6712\n","E0:210\tLoss 3.3469\n","E0:220\tLoss 6.8331\n","E0:230\tLoss 4.8940\n","E0:240\tLoss 46.3101\n","E0:250\tLoss 11.2078\n","E0:260\tLoss 6.8228\n","E0:270\tLoss 27.3504\n","E0:280\tLoss 6.1073\n","E0:290\tLoss 8.8221\n","E0:300\tLoss 11.0132\n","E0:310\tLoss 15.2341\n","E0:320\tLoss 12.7257\n","E0:330\tLoss 9.1849\n","E0:340\tLoss 3.7175\n","E0:350\tLoss 5.7785\n","E0:360\tLoss 5.5908\n","E0:370\tLoss 4.9797\n","E0:380\tLoss 7.8740\n","E0:390\tLoss 9.6222\n","E0:400\tLoss 17.4147\n","E0:410\tLoss 6.7678\n","E0:420\tLoss 9.8785\n","E0:430\tLoss 8.0438\n","E0:440\tLoss 6.6537\n","E0:450\tLoss 8.8321\n","E0:460\tLoss 6.0990\n","E0:470\tLoss 6.9667\n","E0:480\tLoss 4.0177\n","E0:490\tLoss 6.5269\n","E0:500\tLoss 2.7366\n","E0:510\tLoss 9.8213\n","E0:520\tLoss 9.0472\n","E0:530\tLoss 15.6611\n","E0:540\tLoss 4.4270\n","E0:550\tLoss 2.8465\n","E0:560\tLoss 10.1040\n","E0:570\tLoss 5.3352\n","E0:580\tLoss 6.8031\n","E0:590\tLoss 5.9361\n","E0:600\tLoss 17.4173\n","E0:610\tLoss 16.9744\n","E0:620\tLoss 4.2843\n","E0:630\tLoss 4.3456\n","E0:640\tLoss 4.4647\n","E0:650\tLoss 8.4903\n","E0:660\tLoss 4.3776\n","E0:670\tLoss 3.3913\n","E0:680\tLoss 11.0887\n","E0:690\tLoss 3.5859\n","E0:700\tLoss 6.7182\n","E0:710\tLoss 2.0174\n","E0:720\tLoss 7.2114\n","E0:730\tLoss 4.1232\n","E0:740\tLoss 3.8789\n","E0:750\tLoss 8.2088\n","E0:760\tLoss 3.0738\n","E0:770\tLoss 1.8118\n","E0:780\tLoss 8.8448\n","E0:790\tLoss 4.4127\n","E0:800\tLoss 9.2480\n","E0:810\tLoss 5.4822\n","E0:820\tLoss 2.2577\n","E0:830\tLoss 11.7162\n","E0:840\tLoss 7.1575\n","E0:850\tLoss 7.7486\n","E0:860\tLoss 7.6811\n","E0:870\tLoss 3.0672\n","E0:880\tLoss 2.8933\n","E0:890\tLoss 9.2259\n","E0:900\tLoss 4.7664\n","E0:910\tLoss 5.3301\n","E0:920\tLoss 4.5435\n","E0:930\tLoss 8.1135\n","E0:940\tLoss 4.3476\n","E0:950\tLoss 2.6842\n","E0:960\tLoss 7.4299\n","E0:970\tLoss 7.6275\n","E0:980\tLoss 5.1651\n","E0:990\tLoss 7.6698\n","E0:1000\tLoss 3.8249\n","E0:1010\tLoss 5.9146\n","E0:1020\tLoss 6.6230\n","E0:1030\tLoss 3.6160\n","E0:1040\tLoss 5.9057\n","E0:1050\tLoss 2.2902\n","E0:1060\tLoss 3.5748\n","E0:1070\tLoss 8.9272\n","E0:1080\tLoss 4.8985\n","E0:1090\tLoss 3.0793\n","E0:1100\tLoss 3.6912\n","E0:1110\tLoss 4.4477\n","E0:1120\tLoss 4.5432\n","E0:1130\tLoss 2.0704\n","E0:1140\tLoss 3.4520\n","E0:1150\tLoss 2.9210\n","E0:1160\tLoss 6.6592\n","E0:1170\tLoss 9.6938\n","E0:1180\tLoss 3.6046\n","E0:1190\tLoss 5.1251\n","E0:1200\tLoss 2.6011\n","E0:1210\tLoss 3.8274\n","E0:1220\tLoss 4.1197\n","E0:1230\tLoss 2.2906\n","E0:1240\tLoss 2.2512\n","E0:1250\tLoss 7.5546\n","E0:1260\tLoss 9.7012\n","E0:1270\tLoss 2.9428\n","E0:1280\tLoss 2.0940\n","E0:1290\tLoss 9.0990\n","E0:1300\tLoss 3.4347\n","E0:1310\tLoss 2.0273\n","E0:1320\tLoss 8.0069\n","E0:1330\tLoss 2.2643\n","E0:1340\tLoss 3.7987\n","E0:1350\tLoss 3.2376\n","E0:1360\tLoss 4.5362\n","E0:1370\tLoss 3.5556\n","E0:1380\tLoss 2.9020\n","E0:1390\tLoss 3.2942\n","E0:1400\tLoss 2.5880\n","E0:1410\tLoss 15.7274\n","E0:1420\tLoss 2.7491\n","E0:1430\tLoss 3.6987\n","E0:1440\tLoss 9.2398\n","E0:1450\tLoss 6.4870\n","E0:1460\tLoss 10.7710\n","E0:1470\tLoss 5.5235\n","E0:1480\tLoss 5.8634\n","E0:1490\tLoss 2.2715\n","E0:1500\tLoss 11.0312\n","E0:1510\tLoss 3.3237\n","E0:1520\tLoss 2.6950\n","E0:1530\tLoss 3.2517\n","E0:1540\tLoss 3.1285\n","E0:1550\tLoss 7.1463\n","E0:1560\tLoss 6.3196\n","E0:1570\tLoss 4.9801\n","E0:1580\tLoss 2.8900\n","E0:1590\tLoss 3.1217\n","E0:1600\tLoss 3.1753\n","E0:1610\tLoss 2.0182\n","E0:1620\tLoss 1.8516\n","E0:1630\tLoss 3.2473\n","E0:1640\tLoss 1.8671\n","E0:1650\tLoss 3.1702\n","E0:1660\tLoss 7.9567\n","E0:1670\tLoss 4.2825\n","E0:1680\tLoss 50.3898\n","E0:1690\tLoss 6.6058\n","E0:1700\tLoss 3.7332\n","E0:1710\tLoss 1.9886\n","E0:1720\tLoss 1.9694\n","E0:1730\tLoss 3.4203\n","E0:1740\tLoss 11.6673\n","E0:1750\tLoss 4.2621\n","E0:1760\tLoss 4.6777\n","E0:1770\tLoss 2.7638\n","E0:1780\tLoss 2.3256\n","E0:1790\tLoss 8.6286\n","E0:1800\tLoss 2.6300\n","E0:1810\tLoss 30.5286\n","E0:1820\tLoss 4.5873\n","E0:1830\tLoss 9.2371\n","E0:1840\tLoss 2.8675\n","E0:1850\tLoss 7.7447\n","E0:1860\tLoss 5.3381\n","E0:1870\tLoss 3.3962\n","E0:1880\tLoss 9.6074\n","E0:1890\tLoss 10.1083\n","E0:1900\tLoss 12.5390\n","----- EPOCH 1 -----\n","E1:0\tLoss 7.9751\n","E1:10\tLoss 4.9163\n","E1:20\tLoss 4.4079\n","E1:30\tLoss 9.9606\n","E1:40\tLoss 11.9584\n","E1:50\tLoss 4.9177\n","E1:60\tLoss 9.0057\n","E1:70\tLoss 6.9183\n","E1:80\tLoss 5.0828\n","E1:90\tLoss 4.0169\n","E1:100\tLoss 10.2112\n","E1:110\tLoss 3.2221\n","E1:120\tLoss 6.0318\n","E1:130\tLoss 12.5472\n","E1:140\tLoss 11.2360\n","E1:150\tLoss 6.7254\n","E1:160\tLoss 7.8219\n","E1:170\tLoss 8.7400\n","E1:180\tLoss 6.8452\n","E1:190\tLoss 10.3223\n","E1:200\tLoss 8.1912\n","E1:210\tLoss 5.2117\n","E1:220\tLoss 5.8582\n","E1:230\tLoss 14.8142\n","E1:240\tLoss 3.9434\n","E1:250\tLoss 3.3258\n","E1:260\tLoss 3.5889\n","E1:270\tLoss 3.7249\n","E1:280\tLoss 5.8743\n","E1:290\tLoss 6.1608\n","E1:300\tLoss 9.0588\n","E1:310\tLoss 2.9828\n","E1:320\tLoss 3.1728\n","E1:330\tLoss 11.0718\n","E1:340\tLoss 2.6432\n","E1:350\tLoss 2.9428\n","E1:360\tLoss 5.7387\n","E1:370\tLoss 2.5173\n","E1:380\tLoss 5.5514\n","E1:390\tLoss 5.8319\n","E1:400\tLoss 4.3410\n","E1:410\tLoss 4.5151\n","E1:420\tLoss 3.8082\n","E1:430\tLoss 6.2229\n","E1:440\tLoss 2.5884\n","E1:450\tLoss 2.7340\n","E1:460\tLoss 2.8203\n","E1:470\tLoss 2.8276\n","E1:480\tLoss 2.1086\n","E1:490\tLoss 5.4709\n","E1:500\tLoss 3.4978\n","E1:510\tLoss 2.1615\n","E1:520\tLoss 1.6998\n","E1:530\tLoss 12.0263\n","E1:540\tLoss 1.8879\n","E1:550\tLoss 2.7969\n","E1:560\tLoss 2.5655\n","E1:570\tLoss 2.6449\n","E1:580\tLoss 16.5437\n","E1:590\tLoss 9.6925\n","E1:600\tLoss 14.2467\n","E1:610\tLoss 4.1776\n","E1:620\tLoss 4.9482\n","E1:630\tLoss 3.3277\n","E1:640\tLoss 6.3029\n","E1:650\tLoss 2.6855\n","E1:660\tLoss 2.7405\n","E1:670\tLoss 3.2142\n","E1:680\tLoss 4.7183\n","E1:690\tLoss 1.8527\n","E1:700\tLoss 7.1293\n","E1:710\tLoss 2.9583\n","E1:720\tLoss 11.1305\n","E1:730\tLoss 9.8801\n","E1:740\tLoss 3.8830\n","E1:750\tLoss 2.9562\n","E1:760\tLoss 1.8199\n","E1:770\tLoss 2.4270\n","E1:780\tLoss 6.1729\n","E1:790\tLoss 1.2075\n","E1:800\tLoss 1.9314\n","E1:810\tLoss 5.7758\n","E1:820\tLoss 7.5643\n","E1:830\tLoss 2.7439\n","E1:840\tLoss 4.7945\n","E1:850\tLoss 2.7856\n","E1:860\tLoss 2.3777\n","E1:870\tLoss 3.0522\n","E1:880\tLoss 11.5027\n","E1:890\tLoss 10.3483\n","E1:900\tLoss 3.0354\n","E1:910\tLoss 3.8444\n","E1:920\tLoss 3.1438\n","E1:930\tLoss 6.4768\n","E1:940\tLoss 7.2387\n","E1:950\tLoss 2.7528\n","E1:960\tLoss 157.7670\n","E1:970\tLoss 2.6447\n","E1:980\tLoss 7.1199\n","E1:990\tLoss 4.7696\n","E1:1000\tLoss 7.3566\n","E1:1010\tLoss 4.7578\n","E1:1020\tLoss 58.9202\n","E1:1030\tLoss 5.7832\n","E1:1040\tLoss 1.9787\n","E1:1050\tLoss 33.0334\n","E1:1060\tLoss 5.7249\n","E1:1070\tLoss 5.4258\n","E1:1080\tLoss 3.2352\n","E1:1090\tLoss 4.3264\n","E1:1100\tLoss 2.1792\n","E1:1110\tLoss 4.0102\n","E1:1120\tLoss 2.2309\n","E1:1130\tLoss 3.5888\n","E1:1140\tLoss 3.2834\n","E1:1150\tLoss 4.7378\n","E1:1160\tLoss 63.7279\n","E1:1170\tLoss 9.5868\n","E1:1180\tLoss 4.9977\n","E1:1190\tLoss 5.2337\n","E1:1200\tLoss 3.4549\n","E1:1210\tLoss 6.6566\n","E1:1220\tLoss 3.8555\n","E1:1230\tLoss 4.4081\n","E1:1240\tLoss 3.8180\n","E1:1250\tLoss 32.8202\n","E1:1260\tLoss 9.1192\n","E1:1270\tLoss 2.1360\n","E1:1280\tLoss 3.9179\n","E1:1290\tLoss 2.8704\n","E1:1300\tLoss 5.1739\n","E1:1310\tLoss 5.2187\n","E1:1320\tLoss 2.5014\n","E1:1330\tLoss 2.6255\n","E1:1340\tLoss 6.6811\n","E1:1350\tLoss 1.4864\n","E1:1360\tLoss 5.4589\n","E1:1370\tLoss 2.6333\n","E1:1380\tLoss 10.6035\n","E1:1390\tLoss 1.4542\n","E1:1400\tLoss 6.0246\n","E1:1410\tLoss 8.3462\n","E1:1420\tLoss 9.5961\n","E1:1430\tLoss 5.3885\n","E1:1440\tLoss 4.0937\n","E1:1450\tLoss 8.2915\n","E1:1460\tLoss 7.3895\n","E1:1470\tLoss 7.3380\n","E1:1480\tLoss 3.5645\n","E1:1490\tLoss 11.8601\n","E1:1500\tLoss 3.4846\n","E1:1510\tLoss 14.3780\n","E1:1520\tLoss 8.0745\n","E1:1530\tLoss 6.1034\n","E1:1540\tLoss 7.9967\n","E1:1550\tLoss 5.8184\n","E1:1560\tLoss 4.8603\n","E1:1570\tLoss 8.9422\n","E1:1580\tLoss 4.0061\n","E1:1590\tLoss 5.5419\n","E1:1600\tLoss 3.0305\n","E1:1610\tLoss 4.2202\n","E1:1620\tLoss 82.7179\n","E1:1630\tLoss 5.9841\n","E1:1640\tLoss 12.7146\n","E1:1650\tLoss 8.2469\n","E1:1660\tLoss 1.8339\n","E1:1670\tLoss 2.7291\n","E1:1680\tLoss 6.7835\n","E1:1690\tLoss 6.1937\n","E1:1700\tLoss 2.4952\n","E1:1710\tLoss 4.8810\n","E1:1720\tLoss 9.7694\n","E1:1730\tLoss 3.4862\n","E1:1740\tLoss 2.5677\n","E1:1750\tLoss 1.8952\n","E1:1760\tLoss 6.1199\n","E1:1770\tLoss 2.7221\n","E1:1780\tLoss 4.0444\n","E1:1790\tLoss 4.8343\n","E1:1800\tLoss 2.7807\n","E1:1810\tLoss 9.4206\n","E1:1820\tLoss 5.9512\n","E1:1830\tLoss 15.3576\n","E1:1840\tLoss 6.2052\n","E1:1850\tLoss 3.3340\n","E1:1860\tLoss 3.1952\n","E1:1870\tLoss 4.0648\n","E1:1880\tLoss 5.7687\n","E1:1890\tLoss 3.3152\n","E1:1900\tLoss 5.4120\n","----- EPOCH 2 -----\n","E2:0\tLoss 4.1696\n","E2:10\tLoss 5.8326\n","E2:20\tLoss 8.6268\n","E2:30\tLoss 6.1766\n","E2:40\tLoss 4.0111\n","E2:50\tLoss 7.5087\n","E2:60\tLoss 4.6594\n","E2:70\tLoss 3.7208\n","E2:80\tLoss 4.8820\n","E2:90\tLoss 3.4517\n","E2:100\tLoss 4.1080\n","E2:110\tLoss 5.4542\n","E2:120\tLoss 2.0531\n","E2:130\tLoss 8.0252\n","E2:140\tLoss 1.7918\n","E2:150\tLoss 6.0550\n","E2:160\tLoss 11.7305\n","E2:170\tLoss 9.4118\n","E2:180\tLoss 5.7535\n","E2:190\tLoss 2.9817\n","E2:200\tLoss 2.9498\n","E2:210\tLoss 5.3028\n","E2:220\tLoss 1.8446\n","E2:230\tLoss 2.9931\n","E2:240\tLoss 2.5344\n","E2:250\tLoss 6.5694\n","E2:260\tLoss 5.9232\n","E2:270\tLoss 19.1000\n","E2:280\tLoss 4.6459\n","E2:290\tLoss 5.5123\n","E2:300\tLoss 4.2741\n","E2:310\tLoss 4.6102\n","E2:320\tLoss 9.3267\n","E2:330\tLoss 4.4580\n","E2:340\tLoss 3.0940\n","E2:350\tLoss 2.1932\n","E2:360\tLoss 11.4301\n","E2:370\tLoss 4.1790\n","E2:380\tLoss 9.3123\n","E2:390\tLoss 2.9222\n","E2:400\tLoss 11.0140\n","E2:410\tLoss 2.2808\n","E2:420\tLoss 108.4382\n","E2:430\tLoss 4.1967\n","E2:440\tLoss 1.9474\n","E2:450\tLoss 8.5149\n","E2:460\tLoss 8.0821\n","E2:470\tLoss 5.8053\n","E2:480\tLoss 8.5556\n","E2:490\tLoss 3.1235\n","E2:500\tLoss 3.0696\n","E2:510\tLoss 6.9868\n","E2:520\tLoss 2.3066\n","E2:530\tLoss 7.7434\n","E2:540\tLoss 2.4385\n","E2:550\tLoss 3.0744\n","E2:560\tLoss 1.3572\n","E2:570\tLoss 17.7326\n","E2:580\tLoss 25.1674\n","E2:590\tLoss 4.1097\n","E2:600\tLoss 2.8811\n","E2:610\tLoss 5.4582\n","E2:620\tLoss 4.6640\n","E2:630\tLoss 1.4628\n","E2:640\tLoss 1.6731\n","E2:650\tLoss 5.0967\n","E2:660\tLoss 2.2530\n","E2:670\tLoss 4.2398\n","E2:680\tLoss 3.0309\n","E2:690\tLoss 4.3323\n","E2:700\tLoss 2.1244\n","E2:710\tLoss 1.4807\n","E2:720\tLoss 3.0121\n","E2:730\tLoss 9.4150\n","E2:740\tLoss 1.8714\n","E2:750\tLoss 2.5422\n","E2:760\tLoss 1.2292\n","E2:770\tLoss 1.3465\n","E2:780\tLoss 1.6923\n","E2:790\tLoss 13.7106\n","E2:800\tLoss 3.5901\n","E2:810\tLoss 7.1628\n","E2:820\tLoss 1.7333\n","E2:830\tLoss 5.0880\n","E2:840\tLoss 4.2505\n","E2:850\tLoss 2.2689\n","E2:860\tLoss 5.6397\n","E2:870\tLoss 2.4801\n","E2:880\tLoss 5.2412\n","E2:890\tLoss 4.1408\n","E2:900\tLoss 4.9223\n","E2:910\tLoss 3.2900\n","E2:920\tLoss 3.3292\n","E2:930\tLoss 3.3029\n","E2:940\tLoss 1.6409\n","E2:950\tLoss 137.0436\n","E2:960\tLoss 7.8967\n","E2:970\tLoss 2.6926\n","E2:980\tLoss 2.6912\n","E2:990\tLoss 11.7667\n","E2:1000\tLoss 7.1466\n","E2:1010\tLoss 4.8434\n","E2:1020\tLoss 4.4815\n","E2:1030\tLoss 2.9204\n","E2:1040\tLoss 9.1648\n","E2:1050\tLoss 2.6822\n","E2:1060\tLoss 6.7248\n","E2:1070\tLoss 3.8332\n","E2:1080\tLoss 1.9136\n","E2:1090\tLoss 4.9248\n","E2:1100\tLoss 3.3845\n","E2:1110\tLoss 2.7973\n","E2:1120\tLoss 4.6086\n","E2:1130\tLoss 3.2238\n","E2:1140\tLoss 3.8024\n","E2:1150\tLoss 1.9996\n","E2:1160\tLoss 1.4039\n","E2:1170\tLoss 2.9553\n","E2:1180\tLoss 3.1145\n","E2:1190\tLoss 2.6872\n","E2:1200\tLoss 1.4800\n","E2:1210\tLoss 1.8024\n","E2:1220\tLoss 2.0889\n","E2:1230\tLoss 1.9077\n","E2:1240\tLoss 3.4813\n","E2:1250\tLoss 1.2589\n","E2:1260\tLoss 7.3686\n","E2:1270\tLoss 1.8007\n","E2:1280\tLoss 1.9661\n","E2:1290\tLoss 7.5052\n","E2:1300\tLoss 2.1981\n","E2:1310\tLoss 1.7421\n","E2:1320\tLoss 5.1155\n","E2:1330\tLoss 1.6475\n","E2:1340\tLoss 2.8200\n","E2:1350\tLoss 1.3657\n","E2:1360\tLoss 4.1808\n","E2:1370\tLoss 1.8446\n","E2:1380\tLoss 9.5178\n","E2:1390\tLoss 1.6684\n","E2:1400\tLoss 2.2397\n","E2:1410\tLoss 3.6370\n","E2:1420\tLoss 17.1375\n","E2:1430\tLoss 2.6880\n","E2:1440\tLoss 2.0173\n","E2:1450\tLoss 5.5035\n","E2:1460\tLoss 5.9567\n","E2:1470\tLoss 3.1776\n","E2:1480\tLoss 2.2345\n","E2:1490\tLoss 4.2451\n","E2:1500\tLoss 1.8704\n","E2:1510\tLoss 4.2888\n","E2:1520\tLoss 6.4388\n","E2:1530\tLoss 3.5242\n","E2:1540\tLoss 3.8008\n","E2:1550\tLoss 9.0010\n","E2:1560\tLoss 3.2455\n","E2:1570\tLoss 157.8669\n","E2:1580\tLoss 5.6334\n","E2:1590\tLoss 6.8080\n","E2:1600\tLoss 5.3068\n","E2:1610\tLoss 6.6472\n","E2:1620\tLoss 3.2934\n","E2:1630\tLoss 5.4823\n","E2:1640\tLoss 1.9870\n","E2:1650\tLoss 2.9750\n","E2:1660\tLoss 4.0356\n","E2:1670\tLoss 5.9996\n","E2:1680\tLoss 2.6492\n","E2:1690\tLoss 8.3993\n","E2:1700\tLoss 7.9742\n","E2:1710\tLoss 5.4858\n","E2:1720\tLoss 2.0154\n","E2:1730\tLoss 3.4163\n","E2:1740\tLoss 5.5018\n","E2:1750\tLoss 3.3203\n","E2:1760\tLoss 2.3466\n","E2:1770\tLoss 1.9135\n","E2:1780\tLoss 2.5784\n","E2:1790\tLoss 2.0000\n","E2:1800\tLoss 3.5564\n","E2:1810\tLoss 6.8054\n","E2:1820\tLoss 2.6228\n","E2:1830\tLoss 4.9667\n","E2:1840\tLoss 2.1460\n","E2:1850\tLoss 5.5938\n","E2:1860\tLoss 8.7669\n","E2:1870\tLoss 2.4258\n","E2:1880\tLoss 5.9788\n","E2:1890\tLoss 4.9227\n","E2:1900\tLoss 5.3062\n","----- EPOCH 3 -----\n","E3:0\tLoss 1.5590\n","E3:10\tLoss 2.6207\n","E3:20\tLoss 5.1810\n","E3:30\tLoss 2.2881\n","E3:40\tLoss 5.6110\n","E3:50\tLoss 2.4874\n","E3:60\tLoss 1.7528\n","E3:70\tLoss 2.7184\n","E3:80\tLoss 2.0961\n","E3:90\tLoss 2.1185\n","E3:100\tLoss 1.9730\n","E3:110\tLoss 6.9995\n","E3:120\tLoss 3.5585\n","E3:130\tLoss 15.3439\n","E3:140\tLoss 2.5143\n","E3:150\tLoss 3.4040\n","E3:160\tLoss 2.6070\n","E3:170\tLoss 1.7898\n","E3:180\tLoss 2.2970\n","E3:190\tLoss 3.5107\n","E3:200\tLoss 7.1453\n","E3:210\tLoss 45.9847\n","E3:220\tLoss 5.4926\n","E3:230\tLoss 1.5827\n","E3:240\tLoss 3.1709\n","E3:250\tLoss 1.5799\n","E3:260\tLoss 2.3276\n","E3:270\tLoss 2.3904\n","E3:280\tLoss 4.5581\n","E3:290\tLoss 3.7634\n","E3:300\tLoss 3.1916\n","E3:310\tLoss 2.5106\n","E3:320\tLoss 1.8479\n","E3:330\tLoss 3.4491\n","E3:340\tLoss 7.0506\n","E3:350\tLoss 3.2106\n","E3:360\tLoss 2.0428\n","E3:370\tLoss 3.3394\n","E3:380\tLoss 1.7984\n","E3:390\tLoss 1.6306\n","E3:400\tLoss 1.9249\n","E3:410\tLoss 2.3721\n","E3:420\tLoss 4.2887\n","E3:430\tLoss 1.6613\n","E3:440\tLoss 3.1949\n","E3:450\tLoss 2.1462\n","E3:460\tLoss 1.1157\n","E3:470\tLoss 1.3652\n","E3:480\tLoss 1.7703\n","E3:490\tLoss 1.9453\n","E3:500\tLoss 1.8007\n","E3:510\tLoss 3.0622\n","E3:520\tLoss 2.4004\n","E3:530\tLoss 2.6185\n","E3:540\tLoss 2.3410\n","E3:550\tLoss 2.4543\n","E3:560\tLoss 1.3145\n","E3:570\tLoss 1.9611\n","E3:580\tLoss 1.2206\n","E3:590\tLoss 6.8979\n","E3:600\tLoss 1.8504\n","E3:610\tLoss 1.6783\n","E3:620\tLoss 1.5142\n","E3:630\tLoss 2.9769\n","E3:640\tLoss 5.4210\n","E3:650\tLoss 1.3313\n","E3:660\tLoss 3.6616\n","E3:670\tLoss 1.6102\n","E3:680\tLoss 1.8063\n","E3:690\tLoss 3.9338\n","E3:700\tLoss 1.0388\n","E3:710\tLoss 2.0056\n","E3:720\tLoss 2.9369\n","E3:730\tLoss 2.0318\n","E3:740\tLoss 2.0345\n","E3:750\tLoss 2.5716\n","E3:760\tLoss 1.7220\n","E3:770\tLoss 1.7020\n","E3:780\tLoss 1.8637\n","E3:790\tLoss 1.9901\n","E3:800\tLoss 1.3699\n","E3:810\tLoss 1.2716\n","E3:820\tLoss 1.3847\n","E3:830\tLoss 1.5885\n","E3:840\tLoss 4.4628\n","E3:850\tLoss 1.9176\n","E3:860\tLoss 1.2087\n","E3:870\tLoss 5.5625\n","E3:880\tLoss 2.3113\n","E3:890\tLoss 2.9685\n","E3:900\tLoss 2.8945\n","E3:910\tLoss 4.8232\n","E3:920\tLoss 3.3239\n","E3:930\tLoss 2.7214\n","E3:940\tLoss 5.7042\n","E3:950\tLoss 2.8715\n","E3:960\tLoss 3.0716\n","E3:970\tLoss 2.8085\n","E3:980\tLoss 1.3958\n","E3:990\tLoss 2.5371\n","E3:1000\tLoss 1.8829\n","E3:1010\tLoss 9.5121\n","E3:1020\tLoss 2.2062\n","E3:1030\tLoss 1.5554\n","E3:1040\tLoss 1.3884\n","E3:1050\tLoss 4.0988\n","E3:1060\tLoss 1.0850\n","E3:1070\tLoss 1.2030\n","E3:1080\tLoss 1.1336\n","E3:1090\tLoss 2.3176\n","E3:1100\tLoss 0.8572\n","E3:1110\tLoss 1.7642\n","E3:1120\tLoss 4.3311\n","E3:1130\tLoss 3.8324\n","E3:1140\tLoss 2.9746\n","E3:1150\tLoss 5.0985\n","E3:1160\tLoss 2.2673\n","E3:1170\tLoss 5.1927\n","E3:1180\tLoss 2.1184\n","E3:1190\tLoss 1.6667\n","E3:1200\tLoss 2.9669\n","E3:1210\tLoss 3.5360\n","E3:1220\tLoss 3.3699\n","E3:1230\tLoss 3.5392\n","E3:1240\tLoss 2.6896\n","E3:1250\tLoss 0.9737\n","E3:1260\tLoss 2.5724\n","E3:1270\tLoss 1.3827\n","E3:1280\tLoss 1.4795\n","E3:1290\tLoss 1.6637\n","E3:1300\tLoss 2.2391\n","E3:1310\tLoss 1.5448\n","E3:1320\tLoss 2.7300\n","E3:1330\tLoss 1.8912\n","E3:1340\tLoss 1.3870\n","E3:1350\tLoss 1.0869\n","E3:1360\tLoss 3.3155\n","E3:1370\tLoss 1.4687\n","E3:1380\tLoss 1.2067\n","E3:1390\tLoss 5.9031\n","E3:1400\tLoss 4.0473\n","E3:1410\tLoss 4.0843\n","E3:1420\tLoss 2.1364\n","E3:1430\tLoss 1.6440\n","E3:1440\tLoss 4.1927\n","E3:1450\tLoss 3.3072\n","E3:1460\tLoss 22.7375\n","E3:1470\tLoss 3.0320\n","E3:1480\tLoss 3.2196\n","E3:1490\tLoss 9.0014\n","E3:1500\tLoss 1.3773\n","E3:1510\tLoss 3.8539\n","E3:1520\tLoss 3.9678\n","E3:1530\tLoss 4.4817\n","E3:1540\tLoss 1.9917\n","E3:1550\tLoss 2.6751\n","E3:1560\tLoss 2.0392\n","E3:1570\tLoss 2.9675\n","E3:1580\tLoss 4.3624\n","E3:1590\tLoss 1.7476\n","E3:1600\tLoss 1.5693\n","E3:1610\tLoss 14.3117\n","E3:1620\tLoss 1.4997\n","E3:1630\tLoss 2.0322\n","E3:1640\tLoss 2.9251\n","E3:1650\tLoss 2.5226\n","E3:1660\tLoss 1.6906\n","E3:1670\tLoss 2.1411\n","E3:1680\tLoss 2.8367\n","E3:1690\tLoss 3.5729\n","E3:1700\tLoss 1.1947\n","E3:1710\tLoss 3.0058\n","E3:1720\tLoss 3.1242\n","E3:1730\tLoss 1.2300\n","E3:1740\tLoss 2.1030\n","E3:1750\tLoss 1.4667\n","E3:1760\tLoss 1.1581\n","E3:1770\tLoss 4.9833\n","E3:1780\tLoss 2.0811\n","E3:1790\tLoss 2.5340\n","E3:1800\tLoss 2.0245\n","E3:1810\tLoss 2.2047\n","E3:1820\tLoss 1.9358\n","E3:1830\tLoss 4.3465\n","E3:1840\tLoss 5.5646\n","E3:1850\tLoss 2.8920\n","E3:1860\tLoss 2.6891\n","E3:1870\tLoss 2.9574\n","E3:1880\tLoss 3.0131\n","E3:1890\tLoss 33.5401\n","E3:1900\tLoss 2.6056\n","----- EPOCH 4 -----\n","E4:0\tLoss 1.6093\n","E4:10\tLoss 2.3050\n","E4:20\tLoss 0.8480\n","E4:30\tLoss 12.0269\n","E4:40\tLoss 7.2110\n","E4:50\tLoss 2.5394\n","E4:60\tLoss 4.0879\n","E4:70\tLoss 1.5580\n","E4:80\tLoss 1.7943\n","E4:90\tLoss 1.1636\n","E4:100\tLoss 3.3693\n","E4:110\tLoss 2.2610\n","E4:120\tLoss 1.0281\n","E4:130\tLoss 1.3979\n","E4:140\tLoss 2.5997\n","E4:150\tLoss 1.2825\n","E4:160\tLoss 3.2413\n","E4:170\tLoss 2.2581\n","E4:180\tLoss 2.7532\n","E4:190\tLoss 1.5474\n","E4:200\tLoss 4.8444\n","E4:210\tLoss 2.1313\n","E4:220\tLoss 2.1228\n","E4:230\tLoss 2.1931\n","E4:240\tLoss 2.3189\n","E4:250\tLoss 3.6292\n","E4:260\tLoss 1.2494\n","E4:270\tLoss 2.3186\n","E4:280\tLoss 0.7854\n","E4:290\tLoss 2.3377\n","E4:300\tLoss 1.1562\n","E4:310\tLoss 2.0872\n","E4:320\tLoss 1.2129\n","E4:330\tLoss 4.1387\n","E4:340\tLoss 1.2305\n","E4:350\tLoss 6.6998\n","E4:360\tLoss 1.5853\n","E4:370\tLoss 1.7811\n","E4:380\tLoss 1.6744\n","E4:390\tLoss 1.5080\n","E4:400\tLoss 1.3591\n","E4:410\tLoss 1.4516\n","E4:420\tLoss 3.0810\n","E4:430\tLoss 1.0386\n","E4:440\tLoss 1.6027\n","E4:450\tLoss 1.7334\n","E4:460\tLoss 1.5675\n","E4:470\tLoss 1.1258\n","E4:480\tLoss 1.6908\n","E4:490\tLoss 1.2255\n","E4:500\tLoss 1.3537\n","E4:510\tLoss 0.9596\n","E4:520\tLoss 1.9805\n","E4:530\tLoss 4.9829\n","E4:540\tLoss 3.2919\n","E4:550\tLoss 3.8273\n","E4:560\tLoss 2.6589\n","E4:570\tLoss 8.6799\n","E4:580\tLoss 10.1356\n","E4:590\tLoss 7.8072\n","E4:600\tLoss 3.6506\n","E4:610\tLoss 2.7902\n","E4:620\tLoss 5.7464\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"1jbIN1vuYHMX"},"source":["df = pd.DataFrame(loss_history).rolling(250).mean()\n","df.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"5Ny489AxYOpu"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_wobbly, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"IUlhYz_9YQbQ"},"source":["model = model.eval()\n","evaluated_sig = evaluate(evaluation_sig_reverb, evaluation_in_sig, model)\n","Audio(evaluated_sig, rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"c7h2L4Y9jY_s"},"source":["test_loss = evaluate_whole_test_set(model)\n","print(f'Test MSE {test_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7knrBucZPsx"},"source":["# Quantize Model"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"jNUeeEDazjQO"},"source":["# DYNAMIC QUANTIZATION - working but not very effective\n","model = PruneUNet(n_channels=1, n_classes=1).to(DEVICE)\n","model.load_state_dict(T.load(pth+f'model-no-pruning-4.pkl'))\n","\n","quantized_model = T.quantization.quantize_dynamic(\n","    model,\n","    {PrunableConv2d, T.nn.ReLU, PrunableBatchNorm2d, T.nn.MaxPool2d, T.nn.Sequential},\n","    dtype=T.qint8,\n","    inplace=True\n",")\n","\n","T.save(quantized_model.state_dict(), pth+f'pruned-model-quantized-4.pkl')\n","\n","%timeit quantized_model(noisy)\n","%timeit model(noisy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CpKHr3zpjbdY"},"source":["test_loss = evaluate_whole_test_set(quantized_model)\n","print(f'Test MSE {test_loss}')"],"execution_count":null,"outputs":[]}]}